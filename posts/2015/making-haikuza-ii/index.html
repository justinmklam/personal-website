<!doctype html><html lang=en><head><title>The Making of Project Haikuza: Part 2</title><meta property="og:image" content="http://justinmklam.com/imgs/blog-imgs/making-haikuza/VL01D336R8.jpg"><meta property="og:type" content="blog"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-85178257-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-85178257-1')</script><script>var shiftWindow=function(){scrollBy(0,-70)};window.addEventListener("hashchange",shiftWindow);function load(){window.location.hash&&shiftWindow()}</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="When the generator gets a bit smarter, but still not smart enough."><meta name=author content="Justin Lam"><link rel=stylesheet href=/css/bootstrap-modified.min.css><link rel=stylesheet href=/css/jmklam-portfolio.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i|Roboto+Slab|DM+Sans:500&display=swap"><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/atom-one-light.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js></script><link rel=stylesheet href=/css/bootstrap-toc.css></head><body onload=load() data-spy=scroll data-target=#toc><!doctype html><html lang=en><nav class="navbar navbar-inverse navbar-fixed-top" role=navigation><div class=container><div class=navbar-header><button id=hamburger-menu type=button class=navbar-toggle data-toggle=collapse data-target=#bs-example-navbar-collapse-1>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=http://justinmklam.com>Justin Lam</a></div><div class="collapse navbar-collapse" id=bs-example-navbar-collapse-1><ul class="nav navbar-nav"><li><a href=/>Blog</a></li><li><a href=/archives/>Archives</a></li><li><a href=/about/>About</a></li><li><a href=/contact/>Contact</a></li></ul></div></div></nav></html><div class=container id=page-container><div class=row><div class=col-sm-2></div><div class=col-sm-8 id=blog><br><h2 data-toc-skip class=blog-title>The Making of Project Haikuza: Part 2</h2><p class=datestamp-index><span class="fa fa-calendar-o"></span>&nbsp Posted on August 6, 2015
<em>&nbsp &middot &nbsp&nbsp6 min read
&nbsp &middot &nbsp
    <span class=disqus-comment-count data-disqus-url=http://justinmklam.com/posts/2015/making-haikuza-ii/#disqus_thread># comments</span>
&nbsp &middot &nbsp
    
    <a class=blog-tag href=/tags/programming>#programming&nbsp</a></em></p><a href=/imgs/blog-imgs/making-haikuza/VL01D336R8.jpg><img class="img-responsive img-span-row blog-header-img" src=/imgs/blog-imgs/making-haikuza/VL01D336R8.jpg></a><div class="blog-content page-content"><p><em>The format of this series is an outline of my thought process during the development of <a href=/projects/software/haikuza/>@thehaikuza</a>.</em></p><blockquote style=text-align:center>Poetry is hard<br>To write when algorithms<br>Are extremely dumb.</blockquote><p>I dont want my haiku generator to be a vegetarian chef. There&rsquo;s nothing wrong with always making word salad, but eventually it&rsquo;ll have to learn to make fancier things. A poetic risotto would be nice from time to time.</p><p>Leaving @thehaikuza to make complete gibberish wasnt what I had intended. I envisioned my algorithm to be able to reconstruct bad haikus, but definitely not as crappy as the ones it actually made. My idea of bad had more to do with this xkcd comic:</p><div class="row captioned-img"><a href=/imgs/blog-imgs/making-haikuza/ios_keyboard.png><img class="img-responsive img-content" src=/imgs/blog-imgs/making-haikuza/ios_keyboard.png></a><p class=caption>iOS Keyboard Predictions (Source: xkcd.com/1427)</p></div><p>In any event, I needed to try something a little more sophisticated than shoving words into slots where they didn&rsquo;t really fit.</p><h1 id=the-lesser-known-cousin-of-2-chainz>The Lesser Known Cousin of 2 Chainz</h1><p>Every time I Googled a new topic I didnt know about, five more topics were thrown on my plate. Notable keywords that popped up:</p><ul><li>Computational linguistics</li><li>Neural networks</li><li>Sentiment classification</li><li>Bayesian inference</li><li>Tree kernels for semantic role labeling</li><li>Markov chains</li></ul><p>By no means am I suggesting that I understood all of those search terms after reading about them. In fact, I still don&rsquo;t and am constantly trying to wrap my head around those polysyllabic words. (Fun fact: anything above three syllables starts to scare most people.) The takeaway is the last item on the list, which coincidentally is the one I did manage to (somewhat) understand: the Markov chain.</p><h2 id=a-primer-for-markov-chains>A Primer for Markov Chains</h2><p>In technical terms, a Markov chain is any random process that undergoes transitions from one state to another. It&rsquo;s also a memoryless process, meaning that it only cares about its current state and not the states it has previously occupied.</p><p>In less technical terms, a Markov chain is like your tokens position during a game of Monopoly. The next property that your token lands on is random (ie. its a random state transition), and every dice roll is independent of the previous roll (ie. its a memoryless process). Probability determines what the next dice roll is: theres a higher chance of rolling a 7 since there are 6 possible combinations (6/36 = 16.7% probability), whereas theres only one way to roll a 2 or 12 (1/36 = 2.8% probability).</p><p>The marvelous aspect of Markov chains is that they can work with any item, not just numbers! Lets say you wanted to form sentences using Markov chains. Given a corpus of phrases (like the entire works of Shakespeare), a flowchart like the one below can be formed and used to create new phrases. Interestingly enough, this is the basis of prediction engines used on smartphone keyboards! It will learn from your phrasing habits and try to guess what your next word will be, based on how frequently youve typed similar words or phrases (just like the above xkcd comic).</p><div class="row captioned-img"><a href=/imgs/blog-imgs/making-haikuza/chain.png><img class="img-responsive img-content" src=/imgs/blog-imgs/making-haikuza/chain.png></a><p class=caption>Visualization of words forming a Markov chain. (Source: Andrew Cholakian's Blog)</p></div><h2 id=harnessing-its-raw-indisputable-power>Harnessing its Raw, Indisputable Power</h2><p>Using Markov chains would allow me to use song lyrics as a training ground for creating alternate phrases. This would consequently form a probability-based flowchart like the one above, allowing me to generate new sentences by walking through each word state and letting probability determine my next word.</p><p>To understand how to turn this into a programmable scenario, let&rsquo;s take a look at an example. Given a phrase like:</p><blockquote><p>Mo butter, mo better, mo slipperier</p></blockquote><p>A Markov chain algorithm will take triplets of the phrase, use the first two words as the dictionary word, and the third word as the definition. If you&rsquo;re familiar with using dictionaries in Python, the former is known as keys and the latter as values. This would result in the above phrase being translated to:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>(&#39;mo&#39;, &#39;butter&#39;) : [&#39;mo&#39;]
(&#39;butter&#39;, &#39;mo&#39;) : [&#39;better&#39;]
(&#39;mo&#39;, &#39;better&#39;) : [&#39;mo&#39;]
(&#39;better&#39;, &#39;mo&#39;) : [&#39;slipperier&#39;]
</code></pre></div><p>Okay, but this isn&rsquo;t very captivating because there&rsquo;s a 1:1 ratio of keys to values, which means that there will never be any mixing and matching of words since the probability of the next word is always 1. Alternatively, if we have a sentence such as:</p><blockquote><p>Living in a land of butter is like living in a paradise with flying unicorns</p></blockquote><p>Then the resulting dictionary will look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>(&#39;Living&#39;, &#39;in&#39;) :     [&#39;a&#39;]
(&#39;in&#39;, &#39;a&#39;) :          [&#39;land&#39;, &#39;paradise&#39;]
(&#39;a&#39;, &#39;land&#39;) :        [&#39;of&#39;]
(&#39;land&#39;, &#39;of&#39;) :       [&#39;butter&#39;]
(&#39;of&#39;, &#39;butter&#39;) :     [&#39;is&#39;]
(&#39;butter&#39;, &#39;is&#39;) :     [&#39;like&#39;]
(&#39;is&#39;, &#39;like&#39;) :       [&#39;living&#39;]
(&#39;like&#39;, &#39;living&#39;) :   [&#39;in&#39;]
(&#39;living&#39;, &#39;in&#39;) :     [&#39;a&#39;]
(&#39;a&#39;, &#39;paradise&#39;) :    [&#39;with&#39;]
(&#39;paradise&#39;, &#39;with&#39;) : [&#39;flying&#39;]
(&#39;with&#39;, &#39;flying&#39;) :   [&#39;unicorns&#39;]
</code></pre></div><p>The interesting part of this phrase (other than the wildly imaginative scenario) is the second line item, where the key <em>(in, a)</em> has two possible values, either land or paradise. So if I&rsquo;m wandering around a word-based flowchart and come across the pair of words in a, my next word can either be land or paradise. Now, this was just for a 15 word phrase, so imagine the magnitude of choices if lyrics for an entire song was used (where more repetitions are prevalent), or all lyrics from an artists full discography, or even a body of text with a +1,000,000 word count meant for this purpose! More phrases and more words result in more possible combinations, which is excellent news for attempting to create pseudo-random sentences.</p><p>The code below for the Markov chain algorithm was adapted from Agiliqs Blog:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>import random

class Markov(object):

    def __init__(self, string):
        self.cache = {}
        self.words = string
        self.word_size = len(self.words)
        self.database()

        print self.cache

    def triples(self):
        &#34;&#34;&#34; Generates triples from the given data string.
        &#34;&#34;&#34;
        if len(self.words) == 3:
            return

        for i in range(len(self.words) - 2):
            yield (self.words[i], self.words[i+1], self.words[i+2])

    def database(self):
        for w1, w2, w3 in self.triples():
            key = (w1, w2)

            if key in self.cache:
                self.cache[key].append(w3)
            else:
                self.cache[key] = [w3]

    def generate_markov_text(self, size=25):
        seed = random.randint(0, self.word_size-3)
        seed_word, next_word = self.words[seed], self.words[seed+1]
        w1, w2 = seed_word, next_word
        gen_words = []
        for i in xrange(size):
            gen_words.append(w1)
            w1, w2 = w2, random.choice(self.cache[(w1, w2)])
        gen_words.append(w2)
        return &#39; &#39;.join(gen_words)
</code></pre></div><h2 id=teaching-markov-the-art-of-haikus>Teaching Markov the Art of Haikus</h2><p>Time to make some progress in making some beautifully robotic poetry! The plan for @thehaikuza V0.2 is to create a Markov chain dictionary using song lyrics and create randomized phrases from the resulting keys and values. Although this implementation still doesn&rsquo;t involve a proper grammar model, it&rsquo;s an improvement from the previous method because of its more structured approach. By simply mixing and matching phrases that once made sense before, there&rsquo;s a much higher probability that the resulting phrase will also make some level of sense.</p><p>The success rate should now go from laughable to respectfully laughable! Progress is progress.</p><p><em>Check out the full repo on <a href=https://github.com/justinmklam/project-haikuza>Github</a>!</em></p></div><br><div class=text-left><a class=blog-nav-btn href=/><span style=font-size:.83em class="glyphicon glyphicon-menu-left"></span>Recent Posts</a></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){var a,b;if(window.location.hostname=="localhost")return;a=document.createElement('script'),a.type='text/javascript',a.async=!0,b='justinmklam',a.src='//'+b+'.disqus.com/embed.js',(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><div class=col-sm-1><nav id=toc data-toggle=toc data-spy=affix></nav></div></div></div><script src=/js/jquery.js></script><script src=/js/bootstrap.min.js></script><script src=/js/bootstrap-toc.min.js></script><script src=/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src=https://use.fontawesome.com/ccb78cc113.js></script><script id=dsq-count-scr src=//justinmklam.disqus.com/count.js async></script><script>$(document).ready(function(){$('#hamburger-menu').click(function(){$(this).toggleClass('open')})})</script></body><footer><div class="col-lg-8 col-centered"><hr><p class=text-center>&copy; 2021 Justin MK Lam</p></div><div id=amzn-assoc-ad-8dae4bf9-b4e8-4c3d-8d99-cd2028a68840></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=8dae4bf9-b4e8-4c3d-8d99-cd2028a68840"></script></footer></html>